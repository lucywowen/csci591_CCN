{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucywowen/csci591_CCN/blob/main/assignments/assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INkAgt-hTY_-"
      },
      "source": [
        "# Assignment 3 -  Interpret data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTEAadCkTZAA"
      },
      "source": [
        "## *YOUR FULL NAME HERE*\n",
        "Netid: Your netid here\n",
        "\n",
        "*Names of students you worked with on this assignment*: LIST HERE IF APPLICABLE (delete if not)\n",
        "\n",
        "Note: this assignment falls under collaboration Mode 2: Individual Assignment â€“ Collaboration Permitted. Please refer to the syllabus on Canvas for additional information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzkmvTNfTZAA"
      },
      "source": [
        "Instructions for all assignments can be found [here](https://github.com/lucywowen/csci547_ML/blob/main/assignments/_Assignment%20Instructions.ipynb) and in the course syllabus on canvas.\n",
        "\n",
        "Total points in the assignment add up to 90; an additional 10 points are allocated to presentation quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAPvzGDXTZAA"
      },
      "source": [
        "#  Learning Objectives\n",
        "The purpose of this assignment is to demonstrate an understanding into some of the recent concepts we've covered.  We'll work through an encoding model using deep learning and you'll be asked to interpret the output.  You'll also make more progress on your group projects and will be asked to discuss your results.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntZoXcmBTZAB"
      },
      "source": [
        "*Note: for all assignments, write out all equations and math using markdown and [LaTeX](https://tobi.oetiker.ch/lshort/lshort.pdf). For this assignment show ALL math work*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJo_2az4TZAD"
      },
      "source": [
        "# Deep learning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "from scipy.stats import zscore\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE"
      ],
      "metadata": {
        "id": "J63y8zU1M_pd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {},
        "id": "UrOLoT2Fe9ET"
      },
      "outputs": [],
      "source": [
        "#@title Data retrieval and loading\n",
        "import os\n",
        "import hashlib\n",
        "import requests\n",
        "\n",
        "fname = \"W3D4_stringer_oribinned1.npz\"\n",
        "url = \"https://osf.io/683xc/download\"\n",
        "expected_md5 = \"436599dfd8ebe6019f066c38aed20580\"\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "  except requests.ConnectionError:\n",
        "    print(\"!!! Failed to download data !!!\")\n",
        "  else:\n",
        "    if r.status_code != requests.codes.ok:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
        "      print(\"!!! Data download appears corrupted !!!\")\n",
        "    else:\n",
        "      with open(fname, \"wb\") as fid:\n",
        "        fid.write(r.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Helper Functions\n",
        "\n",
        "def load_data(data_name, bin_width=1):\n",
        "  \"\"\"Load mouse V1 data from Stringer et al. (2019)\n",
        "\n",
        "  Data from study reported in this preprint:\n",
        "  https://www.biorxiv.org/content/10.1101/679324v2.abstract\n",
        "\n",
        "  These data comprise time-averaged responses of ~20,000 neurons\n",
        "  to ~4,000 stimulus gratings of different orientations, recorded\n",
        "  through Calcium imaginge. The responses have been normalized by\n",
        "  spontaneous levels of activity and then z-scored over stimuli, so\n",
        "  expect negative numbers. They have also been binned and averaged\n",
        "  to each degree of orientation.\n",
        "\n",
        "  This function returns the relevant data (neural responses and\n",
        "  stimulus orientations) in a torch.Tensor of data type torch.float32\n",
        "  in order to match the default data type for nn.Parameters in\n",
        "  Google Colab.\n",
        "\n",
        "  This function will actually average responses to stimuli with orientations\n",
        "  falling within bins specified by the bin_width argument. This helps\n",
        "  produce individual neural \"responses\" with smoother and more\n",
        "  interpretable tuning curves.\n",
        "\n",
        "  Args:\n",
        "    bin_width (float): size of stimulus bins over which to average neural\n",
        "      responses\n",
        "\n",
        "  Returns:\n",
        "    resp (torch.Tensor): n_stimuli x n_neurons matrix of neural responses,\n",
        "        each row contains the responses of each neuron to a given stimulus.\n",
        "        As mentioned above, neural \"response\" is actually an average over\n",
        "        responses to stimuli with similar angles falling within specified bins.\n",
        "    stimuli: (torch.Tensor): n_stimuli x 1 column vector with orientation\n",
        "        of each stimulus, in degrees. This is actually the mean orientation\n",
        "        of all stimuli in each bin.\n",
        "\n",
        "  \"\"\"\n",
        "  with np.load(data_name) as dobj:\n",
        "    data = dict(**dobj)\n",
        "  resp = data['resp']\n",
        "  stimuli = data['stimuli']\n",
        "\n",
        "  if bin_width > 1:\n",
        "    # Bin neural responses and stimuli\n",
        "    bins = np.digitize(stimuli, np.arange(0, 360 + bin_width, bin_width))\n",
        "    stimuli_binned = np.array([stimuli[bins == i].mean() for i in np.unique(bins)])\n",
        "    resp_binned = np.array([resp[bins == i, :].mean(0) for i in np.unique(bins)])\n",
        "  else:\n",
        "    resp_binned = resp\n",
        "    stimuli_binned = stimuli\n",
        "\n",
        "  # only use stimuli <= 180\n",
        "  resp_binned = resp_binned[stimuli_binned <= 180]\n",
        "  stimuli_binned = stimuli_binned[stimuli_binned <= 180]\n",
        "\n",
        "  stimuli_binned -= 90  # 0 means vertical, -ve means tilted left, +ve means tilted right\n",
        "\n",
        "  # Return as torch.Tensor\n",
        "  resp_tensor = torch.tensor(resp_binned, dtype=torch.float32)\n",
        "  stimuli_tensor = torch.tensor(stimuli_binned, dtype=torch.float32).unsqueeze(1)  # add singleton dimension to make a column vector\n",
        "\n",
        "  return resp_tensor, stimuli_tensor\n",
        "\n",
        "\n",
        "def grating(angle, sf=1 / 28, res=0.1, patch=False):\n",
        "  \"\"\"Generate oriented grating stimulus\n",
        "\n",
        "  Args:\n",
        "    angle (float): orientation of grating (angle from vertical), in degrees\n",
        "    sf (float): controls spatial frequency of the grating\n",
        "    res (float): resolution of image. Smaller values will make the image\n",
        "      smaller in terms of pixels. res=1.0 corresponds to 640 x 480 pixels.\n",
        "    patch (boolean): set to True to make the grating a localized\n",
        "      patch on the left side of the image. If False, then the\n",
        "      grating occupies the full image.\n",
        "\n",
        "  Returns:\n",
        "    torch.Tensor: (res * 480) x (res * 640) pixel oriented grating image\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  angle = np.deg2rad(angle)  # transform to radians\n",
        "\n",
        "  wpix, hpix = 640, 480  # width and height of image in pixels for res=1.0\n",
        "\n",
        "  xx, yy = np.meshgrid(sf * np.arange(0, wpix * res) / res, sf * np.arange(0, hpix * res) / res)\n",
        "\n",
        "  if patch:\n",
        "    gratings = np.cos(xx * np.cos(angle + .1) + yy * np.sin(angle + .1))  # phase shift to make it better fit within patch\n",
        "    gratings[gratings < 0] = 0\n",
        "    gratings[gratings > 0] = 1\n",
        "    xcent = gratings.shape[1] * .75\n",
        "    ycent = gratings.shape[0] / 2\n",
        "    xxc, yyc = np.meshgrid(np.arange(0, gratings.shape[1]), np.arange(0, gratings.shape[0]))\n",
        "    icirc = ((xxc - xcent) ** 2 + (yyc - ycent) ** 2) ** 0.5 < wpix / 3 / 2 * res\n",
        "    gratings[~icirc] = 0.5\n",
        "\n",
        "  else:\n",
        "    gratings = np.cos(xx * np.cos(angle) + yy * np.sin(angle))\n",
        "    gratings[gratings < 0] = 0\n",
        "    gratings[gratings > 0] = 1\n",
        "\n",
        "  # Return torch tensor\n",
        "  return torch.tensor(gratings, dtype=torch.float32)\n",
        "\n",
        "\n",
        "def filters(out_channels=6, K=7):\n",
        "  \"\"\" make example filters, some center-surround and gabors\n",
        "  Returns:\n",
        "      filters: out_channels x K x K\n",
        "  \"\"\"\n",
        "  grid = np.linspace(-K/2, K/2, K).astype(np.float32)\n",
        "  xx,yy = np.meshgrid(grid, grid, indexing='ij')\n",
        "\n",
        "  # create center-surround filters\n",
        "  sigma = 1.1\n",
        "  gaussian = np.exp(-(xx**2 + yy**2)**0.5/(2*sigma**2))\n",
        "  wide_gaussian = np.exp(-(xx**2 + yy**2)**0.5/(2*(sigma*2)**2))\n",
        "  center_surround = gaussian - 0.5 * wide_gaussian\n",
        "\n",
        "  # create gabor filters\n",
        "  thetas = np.linspace(0, 180, out_channels-2+1)[:-1] * np.pi/180\n",
        "  gabors = np.zeros((len(thetas), K, K), np.float32)\n",
        "  lam = 10\n",
        "  phi = np.pi/2\n",
        "  gaussian = np.exp(-(xx**2 + yy**2)**0.5/(2*(sigma*0.4)**2))\n",
        "  for i,theta in enumerate(thetas):\n",
        "    x = xx*np.cos(theta) + yy*np.sin(theta)\n",
        "    gabors[i] = gaussian * np.cos(2*np.pi*x/lam + phi)\n",
        "\n",
        "  filters = np.concatenate((center_surround[np.newaxis,:,:],\n",
        "                            -1*center_surround[np.newaxis,:,:],\n",
        "                            gabors),\n",
        "                           axis=0)\n",
        "  filters /= np.abs(filters).max(axis=(1,2))[:,np.newaxis,np.newaxis]\n",
        "  # convert to torch\n",
        "  filters = torch.from_numpy(filters)\n",
        "  # add channel axis\n",
        "  filters = filters.unsqueeze(1)\n",
        "\n",
        "  return filters\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  \"\"\"Deep convolutional network with one convolutional + pooling layer followed\n",
        "  by one fully connected layer\n",
        "\n",
        "  Args:\n",
        "    h_in (int): height of input image, in pixels (i.e. number of rows)\n",
        "    w_in (int): width of input image, in pixels (i.e. number of columns)\n",
        "\n",
        "  Attributes:\n",
        "    conv (nn.Conv2d): filter weights of convolutional layer\n",
        "    pool (nn.MaxPool2d): max pooling layer\n",
        "    dims (tuple of ints): dimensions of output from pool layer\n",
        "    fc (nn.Linear): weights and biases of fully connected layer\n",
        "    out (nn.Linear): weights and biases of output layer\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, h_in, w_in):\n",
        "    super().__init__()\n",
        "    C_in = 1  # input stimuli have only 1 input channel\n",
        "    C_out = 6  # number of output channels (i.e. of convolutional kernels to convolve the input with)\n",
        "    K = 7  # size of each convolutional kernel\n",
        "    Kpool = 8  # size of patches over which to pool\n",
        "    self.conv = nn.Conv2d(C_in, C_out, kernel_size=K, padding=K//2)  # add padding to ensure that each channel has same dimensionality as input\n",
        "    self.pool = nn.MaxPool2d(Kpool)\n",
        "    self.dims = (C_out, h_in // Kpool, w_in // Kpool)  # dimensions of pool layer output\n",
        "    self.fc = nn.Linear(np.prod(self.dims), 10)  # flattened pool output --> 10D representation\n",
        "    self.out = nn.Linear(10, 1)  # 10D representation --> scalar\n",
        "    self.conv.weight = nn.Parameter(filters(C_out, K))\n",
        "    self.conv.bias = nn.Parameter(torch.zeros((C_out,), dtype=torch.float32))\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Classify grating stimulus as tilted right or left\n",
        "\n",
        "    Args:\n",
        "      x (torch.Tensor): p x 48 x 64 tensor with pixel grayscale values for\n",
        "          each of p stimulus images.\n",
        "\n",
        "    Returns:\n",
        "      torch.Tensor: p x 1 tensor with network outputs for each input provided\n",
        "          in x. Each output should be interpreted as the probability of the\n",
        "          corresponding stimulus being tilted right.\n",
        "\n",
        "    \"\"\"\n",
        "    x = x.unsqueeze(1)  # p x 1 x 48 x 64, add a singleton dimension for the single stimulus channel\n",
        "    x = torch.relu(self.conv(x))  # output of convolutional layer\n",
        "    x = self.pool(x)  # output of pooling layer\n",
        "    x = x.view(-1, np.prod(self.dims))  # flatten pooling layer outputs into a vector\n",
        "    x = torch.relu(self.fc(x))  # output of fully connected layer\n",
        "    x = torch.sigmoid(self.out(x))  # network output\n",
        "    return x\n",
        "\n",
        "\n",
        "def train(net, train_data, train_labels,\n",
        "          n_epochs=25, learning_rate=0.0005,\n",
        "          batch_size=100, momentum=.99):\n",
        "  \"\"\"Run stochastic gradient descent on binary cross-entropy loss for a given\n",
        "  deep network (cf. appendix for details)\n",
        "\n",
        "  Args:\n",
        "    net (nn.Module): deep network whose parameters to optimize with SGD\n",
        "    train_data (torch.Tensor): n_train x h x w tensor with stimulus gratings\n",
        "    train_labels (torch.Tensor): n_train x 1 tensor with true tilt of each\n",
        "      stimulus grating in train_data, i.e. 1. for right, 0. for left\n",
        "    n_epochs (int): number of times to run SGD through whole training data set\n",
        "    batch_size (int): number of training data samples in each mini-batch\n",
        "    learning_rate (float): learning rate to use for SGD updates\n",
        "    momentum (float): momentum parameter for SGD updates\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize binary cross-entropy loss function\n",
        "  loss_fn = nn.BCELoss()\n",
        "\n",
        "  # Initialize SGD optimizer with momentum\n",
        "  optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "  # Placeholder to save loss at each iteration\n",
        "  track_loss = []\n",
        "\n",
        "  # Loop over epochs\n",
        "  for i in range(n_epochs):\n",
        "\n",
        "    # Split up training data into random non-overlapping mini-batches\n",
        "    ishuffle = torch.randperm(train_data.shape[0])  # random ordering of training data\n",
        "    minibatch_data = torch.split(train_data[ishuffle], batch_size)  # split train_data into minibatches\n",
        "    minibatch_labels = torch.split(train_labels[ishuffle], batch_size)  # split train_labels into minibatches\n",
        "\n",
        "    # Loop over mini-batches\n",
        "    for stimuli, tilt in zip(minibatch_data, minibatch_labels):\n",
        "\n",
        "      # Evaluate loss and update network weights\n",
        "      out = net(stimuli)  # predicted probability of tilt right\n",
        "      loss = loss_fn(out, tilt)  # evaluate loss\n",
        "      optimizer.zero_grad()  # clear gradients\n",
        "      loss.backward()  # compute gradients\n",
        "      optimizer.step()  # update weights\n",
        "\n",
        "      # Keep track of loss at each iteration\n",
        "      track_loss.append(loss.item())\n",
        "\n",
        "    # Track progress\n",
        "    if (i + 1) % (n_epochs // 5) == 0:\n",
        "      print(f'epoch {i + 1} | loss on last mini-batch: {loss.item(): .2e}')\n",
        "\n",
        "  print('training done!')\n",
        "\n",
        "\n",
        "def get_hidden_activity(net, stimuli, layer_labels):\n",
        "  \"\"\"Retrieve internal representations of network\n",
        "\n",
        "  Args:\n",
        "    net (nn.Module): deep network\n",
        "    stimuli (torch.Tensor): p x 48 x 64 tensor with stimuli for which to\n",
        "      compute and retrieve internal representations\n",
        "    layer_labels (list): list of strings with labels of each layer for which\n",
        "      to return its internal representations\n",
        "\n",
        "  Returns:\n",
        "    dict: internal representations at each layer of the network, in\n",
        "      numpy arrays. The keys of this dict are the strings in layer_labels.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Placeholder\n",
        "  hidden_activity = {}\n",
        "\n",
        "  # Attach 'hooks' to each layer of the network to store hidden\n",
        "  # representations in hidden_activity\n",
        "  def hook(module, input, output):\n",
        "    module_label = list(net._modules.keys())[np.argwhere([module == m for m in net._modules.values()])[0, 0]]\n",
        "    if module_label in layer_labels:  # ignore output layer\n",
        "      hidden_activity[module_label] = output.view(stimuli.shape[0], -1).detach().numpy()\n",
        "  hooks = [layer.register_forward_hook(hook) for layer in net.children()]\n",
        "\n",
        "  # Run stimuli through the network\n",
        "  pred = net(stimuli)\n",
        "\n",
        "  # Remove the hooks\n",
        "  [h.remove() for h in hooks]\n",
        "\n",
        "  return hidden_activity\n",
        "\n",
        "def RDM(resp):\n",
        "  \"\"\"Compute the representational dissimilarity matrix (RDM)\n",
        "\n",
        "  Args:\n",
        "    resp (ndarray): S x N matrix with population responses to\n",
        "      each stimulus in each row\n",
        "\n",
        "  Returns:\n",
        "    ndarray: S x S representational dissimilarity matrix\n",
        "  \"\"\"\n",
        "\n",
        "  # z-score responses to each stimulus\n",
        "  zresp = zscore(resp, axis=1)\n",
        "\n",
        "  # Compute RDM\n",
        "  RDM = 1 - (zresp @ zresp.T) / zresp.shape[1]\n",
        "\n",
        "  return RDM"
      ],
      "metadata": {
        "id": "7rzR1BiLMr52"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeMy78fGTZAD"
      },
      "source": [
        "## 1\n",
        "**[30 points]**\n",
        "\n",
        "\n",
        "**Data**. We will be buidling upon the decoding model we created in class.\n",
        " The data for this problem will be the neural activity in the mouse primary visual cortex responding oriented gratings, collected by [Stringer, et al., 2021](https://www.biorxiv.org/content/10.1101/679324v2.abstract).\n",
        "\n",
        "\n",
        "\n",
        "**Your objective**. For this dataset, your goal is to recreate (and possibly improve!) a decoding model and then create an encoding model. I'll ask for interpretation (in plain english) about your results along the way. BTW if you need extra support and want to watch some videos, follow along with [neuromatch](https://compneuro.neuromatch.io/tutorials/W1D5_DeepLearning/student/W1D5_Tutorial3.html). Let's break it down:\n",
        "\n",
        "a. First load in the data. You've already explored this data a bit in class, but it would be good to get know the data again.  Start by plotting some neural activity in response to stimulus orientation.  What do these plots tell us?  Please describe the data in words.     \n",
        "\n",
        "b. Implement a decoding model for the neural responses.  You can start with same model used in our first neural networks hackathon and train the network using the `resp_train` and `stimuli_train`.\n",
        "\n",
        "- Start by evaluating its initial performance. Get neural responses (`r`) and orientation (`ori`) to one stimulus in dataset using `r, ori = get_data(1, resp_train, stimuli_train)` and then decode the orientation from these neural responses using the network we developed in class. What do you think?  Is that a good prediction?\n",
        "\n",
        "- Ok now look at performance using the test data instead. Make orientation predictions using `resp_test` and `stimuli_test` instead and evalulate the loss. What kinds of conclusions can we draw from these sorts of analyses?\n",
        "\n",
        "- Bonus +1: Attempt to improve those predictions. You can try changing the architecture and adding regularization. You can even explore some other optimization techniques. How do these choices change your loss results?  What are some tradeoffs to consider when varying the neural network architecture?  For the student with the lowest MSE, you'll receive +5 points!\n",
        "\n",
        "c. Now implement an encoding model!  In class, we went through building a convolutional network meant to represent the responses of neurons in the mouse visual cortex, but now we will expand on this and train the model to predict whether the orientation is left or right.  Our goal is to see if the convolutional filters it learns are similar to the mouse visual cortex. We'll start by optimizing the parameters to solve this orientation discrimination task (building the normative model).  We will try an architecture with 1 convolutional layer (which convolves the images with a set of filters) and 1 fully connected layer (which transforms the output of this convolution into a 10-dimensional respresenation). Finally, a set of output weights transforms this 10-dimensional representation into a single scalar, denoting the predicted probability of the input stimulus being tilted right. You can use the given class `CCN` for this (see helper functions) but you can also play around with this architecture. Ready? OK!\n",
        "\n",
        "1. After initializing this `CNN` model, build a dataset of oriented grating stimuli to use for training it (see code below). Pass this into a function called `train()` (see helper functions) that uses `SGD` (stochastic gradient descent) to optimize the model parameters.  This function takes similar arguments as the `train()` function we wrote for the in-class hackathon, but will be different.  Make sure your using the correct `train()` function!\n",
        "\n",
        "2. Next, using the data from [Stringer, et al., 2021](https://www.biorxiv.org/content/10.1101/679324v2.abstract), we will extract our deep `CNN` model representations of these same stimuli (i.e. oriented gratings with the orientations in `ori`). We will run the same stimuli through our `CNN` model and use the helper function `get_hidden_activity()` to store the model internal representations. The output of this function is a Python `dict`, which contains a matrix of population responses (just like `resp_v1`) for each layer of the network specified by the `layer_labels` argument. We will focus on looking at the representations in the output of the first convolutional layer, stored in the model as `'pool'` and the 10-dimensional output of the fully connected layer, stored in the model as `'fc'`.  (See code below)\n",
        "\n",
        "d. We know many mammalian visual systems are capable of solving this orientation discrimination task. It is therefore conceivable that the representations in a deep network model optimized for this task might resemble those in the brain. To test this hypothesis, we will compare the representations of our normative encoding model to neural activity recorded in response to these very same stimuli. To quantify these comparisons, we will use Representational Similarity Analysis (RSA...  but really we'll be looking at the *Dissimilarity*, RDM).  Using the helper function `RDM()`, calculate the RDM of the population responses in the V1 data and in each layer of our model `CNN`: `rdm_dict = {label: RDM(resp) for label, resp in resp_dict.items()}`.  Try plotting these dissimilarities. What structure do you notice in these plots? Now compute the correlation between the RDMs for each layer of our model CNN and that of the V1 data.  What is do these correlations mean in terms of quantitatively comparing the model neurons and the brain neurons? Which layerâ€™s representations most resemble those in the data?\n",
        "\n",
        "e. Finally, let's try to better understand the representations in the data and in each of the model layers.  \n",
        "1. First, plot the responses of a single neuron (or units in the model) as a function of the stimulus orientation.  What's is this called (HINT we've discussed this in class quite a bit).  How are the single neuron responses similar/different between the model and the data?  \n",
        "2. Extra credit +2 Try visualizing a dimensionality-reduced version of the internal representations (V1 responses) and `CNN` internal representations to explore potentially informative structure.  You can use any dimensionality technique you'd like but please plot a few of your results.  Then try to interpret what you see...  how are the population responses similar/different between the model and the data? How can you use what you know from e1 to inform these interpretations?  How do the representations in the different layers of the model differ, and how does this relate to the orientation discrimination task the model was optimized for? Which layer of our deep network encoding model most closely resembles the V1 data?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Code for part C1\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(12)\n",
        "torch.manual_seed(12)\n",
        "\n",
        "# Set height and width of stimulus\n",
        "h_ = 3\n",
        "h, w  = grating(0).shape\n",
        "\n",
        "# Initialize CNN model with height and width of stimulus\n",
        "net = CNN(h, w)\n",
        "\n",
        "# Build training set to train it on\n",
        "n_train = 1000  # size of training set\n",
        "\n",
        "# sample n_train random orientations between -90 and +90 degrees\n",
        "ori = (np.random.rand(n_train) - 0.5) * 180\n",
        "\n",
        "# build orientated grating stimuli\n",
        "stimuli = torch.stack([grating(i) for i in ori])\n",
        "\n",
        "# stimulus tilt: 1. if tilted right, 0. if tilted left, as a column vector\n",
        "tilt = torch.tensor(ori > 0).type(torch.float).unsqueeze(-1)\n",
        "\n",
        "# Train model\n",
        "train(net, stimuli, tilt)"
      ],
      "metadata": {
        "id": "39kozzWTiGPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Code for part C2\n",
        "# Load mouse V1 data\n",
        "resp_v1, ori = load_data(fname)\n",
        "\n",
        "# Extract model internal representations of each stimulus in the V1 data\n",
        "# construct grating stimuli for each orientation presented in the V1 data\n",
        "stimuli = torch.stack([grating(a.item()) for a in ori])\n",
        "layer_labels = ['pool', 'fc']\n",
        "resp_model = get_hidden_activity(net, stimuli, layer_labels)\n",
        "\n",
        "# Aggregate all responses into one dict\n",
        "resp_dict = {}\n",
        "resp_dict['V1 data'] = resp_v1\n",
        "for k, v in resp_model.items():\n",
        "  label = f\"model\\n'{k}' layer\"\n",
        "  resp_dict[label] = v"
      ],
      "metadata": {
        "id": "gfGcHWqRQ1tU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFX1SSqATZAD"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gmy53UgnIOgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uENxx9_VTZAD"
      },
      "source": [
        "# Project interpretation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shzvGVzgTZAE"
      },
      "source": [
        "## 2\n",
        "**[60 points]**\n",
        "\n",
        "**Data**. The data you'll use for this section will be related to your project (either the data you're working on with your group or a different dataset related to your question).  \n",
        "\n",
        "**Objective**. For this you'll continue to build on your analyses and results from your group project, but I would like you to *discuss* your results. You can work with your group on this, but you should each submit a different analyses and discussion.  \n",
        "\n",
        "I am purposefully leaving this prompt open-ended.  By now I think you all can handle an analysis. If it is a similar analysis to the one you submitted for Assignment 2, please show some progress or model improvements (comparing results using cross validation).  Please still submit the code for your analysis, but what I am wanting is an in-depth discussion of the results.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BASw0yPhTZAE"
      },
      "source": [
        "**ANSWER**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-8f81sObiAS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "nteract": {
      "version": "0.28.0"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "643px",
        "left": "1548px",
        "right": "20px",
        "top": "121px",
        "width": "350px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}