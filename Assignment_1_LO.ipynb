{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucywowen/csci591_CCN/blob/main/Assignment_1_LO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INkAgt-hTY_-"
      },
      "source": [
        "# Assignment 1 -  & Computational Programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTEAadCkTZAA"
      },
      "source": [
        "## *YOUR FULL NAME HERE*\n",
        "Netid: Your netid here\n",
        "\n",
        "*Names of students you worked with on this assignment*: LIST HERE IF APPLICABLE (delete if not)\n",
        "\n",
        "Note: this assignment falls under collaboration Mode 2: Individual Assignment â€“ Collaboration Permitted. Please refer to the syllabus on Canvas for additional information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzkmvTNfTZAA"
      },
      "source": [
        "Instructions for all assignments can be found [here](https://github.com/kylebradbury/ids705/blob/master/assignments/_Assignment%20Instructions.ipynb), and is also linked to from the [course syllabus](https://kylebradbury.github.io/ids705/index.html).\n",
        "\n",
        "Total points in the assignment add up to 90; an additional 10 points are allocated to presentation quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAPvzGDXTZAA"
      },
      "source": [
        "#  Learning Objectives\n",
        "The purpose of this assignment is to provide a refresher on fundamental concepts that we will use throughout this course, and provide an opportunity to develop skills in any of the related skills that may be unfamiliar to you. Through the course of completing this assignment, you will...\n",
        "- Refresh you knowledge of probability theory including properties of random variables, probability density functions,  cumulative distribution functions, and key statistics such as mean and variance.\n",
        "- Revisit common linear algebra and matrix operations and concepts such as matrix multiplication, inner and outer products, inverses, the Hadamard (element-wise) product, eigenvalues and eigenvectors, orthogonality, and symmetry.\n",
        "- Practice numerical programming, core to machine learning, by loading and filtering data, plotting data, vectorizing operations, profiling code speed, and debugging and optimizing performance. You will also practice computing probabilities based on simulation.\n",
        "- Develop or refresh your knowledge of Git version control, which will be a core tool used in the final project of this course\n",
        "- Apply your skills altogether through an exploratory data analysis to practice data cleaning, data manipulation, interpretation, and communication\n",
        "\n",
        "We will build on these concepts throughout the course, so use this assignment as a catalyst to deepen your knowledge and seek help with anything that is unfamiliar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJhs3OZOTZAB"
      },
      "source": [
        "# Probability and Statistics Theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntZoXcmBTZAB"
      },
      "source": [
        "*Note: for all assignments, write out all equations and math using markdown and [LaTeX](https://tobi.oetiker.ch/lshort/lshort.pdf). For this assignment show ALL math work*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNhy-v8kTZAC"
      },
      "source": [
        "## 1\n",
        "**[3 points]**  \n",
        "Let $f(x) = \\begin{cases}\n",
        "                0           & x < 0  \\\\\n",
        "                \\alpha x^2  & 0 \\leq x \\leq 2 \\\\\n",
        "                0           & 2 < x\n",
        "            \\end{cases}$\n",
        "            \n",
        "For what value of $\\alpha$ is $f(x)$ a valid probability density function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebr_TgSATZAC"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRkH9biQTZAC"
      },
      "source": [
        "## 2\n",
        "**[3 points]** What is the cumulative distribution function (CDF) that corresponds to the following probability distribution function? Please state the value of the CDF for all possible values of $x$.\n",
        "\n",
        "$f(x) = \\begin{cases}\n",
        "    \\frac{1}{3} & 0 < x < 3 \\\\\n",
        "    0           & \\text{otherwise}\n",
        "    \\end{cases}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UdT7YkLTZAC"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_9YMv1bTZAC"
      },
      "source": [
        "## 3\n",
        "**[6 points]** For the probability distribution function for the random variable $X$,\n",
        "\n",
        "$f(x) = \\begin{cases}\n",
        "    \\frac{1}{3} & 0 < x < 3 \\\\\n",
        "    0           & \\text{otherwise}\n",
        "    \\end{cases}$\n",
        "    \n",
        "what is the (a) expected value and (b) variance of $X$. *Show all work*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj5A1RcUTZAC"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-krXRNdvTZAC"
      },
      "source": [
        "## 4\n",
        "**[6 points]** Consider the following table of data that provides the values of a discrete data vector $\\mathbf{x}$ of samples from the random variable $X$, where each entry in $\\mathbf{x}$ is given as $x_i$.\n",
        "\n",
        "*Table 1. Dataset N=5 observations*\n",
        "\n",
        "|        | $x_0$ | $x_1$ | $x_2$ | $x_3$ | $x_4$ |\n",
        "|------  |-------|-------|-------|-------|-------|\n",
        "|$\\textbf{x}$| 2     | 3     | 10    | -1    | -1    |\n",
        "\n",
        "What is the (a) mean and (b) variance of the data?\n",
        "\n",
        "*Show all work. Your answer should include the definition of mean and variance in the context of discrete data. In this case, use the sample variance since the sample size is quite small*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmYDqeVoTZAC"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQIqPndpTZAD"
      },
      "source": [
        "# Linear Algebra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqoxWu5OTZAD"
      },
      "source": [
        "## 5\n",
        "**[14 points]** **Matrix manipulations and multiplication**. Machine learning involves working with many matrices, so this exercise will provide you with the opportunity to practice those skills.\n",
        "\n",
        "Let\n",
        "$\\mathbf{A} =  \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "2 & 4 & 5 \\\\\n",
        "3 & 5 & 6\n",
        "\\end{bmatrix}$, $\\mathbf{b} =  \\begin{bmatrix}\n",
        "-1  \\\\\n",
        "3  \\\\\n",
        "8  \n",
        "\\end{bmatrix}$, $\\mathbf{c} =  \\begin{bmatrix}\n",
        "4  \\\\\n",
        "-3  \\\\\n",
        "6  \n",
        "\\end{bmatrix}$, and $\\mathbf{I} =  \\begin{bmatrix}\n",
        "1 & 0 & 0 \\\\\n",
        "0 & 1 & 0 \\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}$\n",
        "\n",
        "Compute the following (using Python) or indicate that it cannot be computed. Refer to numpy's tools for handling matrices.\n",
        "\n",
        "1. $\\mathbf{A}\\mathbf{A}$\n",
        "2. $\\mathbf{A}\\mathbf{A}^T$\n",
        "3. $\\mathbf{A}\\mathbf{b}$\n",
        "4. $\\mathbf{A}\\mathbf{b}^T$\n",
        "5. $\\mathbf{b}\\mathbf{A}$\n",
        "6. $\\mathbf{b}^T\\mathbf{A}$\n",
        "7. $\\mathbf{b}\\mathbf{b}$\n",
        "8. $\\mathbf{b}^T\\mathbf{b}$\n",
        "9. $\\mathbf{b}\\mathbf{b}^T$\n",
        "10. $\\mathbf{b} + \\mathbf{c}^T$\n",
        "11. $\\mathbf{b}^T\\mathbf{b}^T$\n",
        "12. $\\mathbf{A}^{-1}\\mathbf{b}$\n",
        "13. $\\mathbf{A}\\circ\\mathbf{A}$\n",
        "14. $\\mathbf{b}\\circ\\mathbf{c}$\n",
        "\n",
        "*Note: The element-wise (or Hadamard) product is the product of each element in one matrix with the corresponding element in another matrix, and is represented by the symbol \"$\\circ$\".*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgTric8uTZAD"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA1hEV73TZAD"
      },
      "source": [
        "## 6\n",
        "**[8 points]** **Eigenvectors and eigenvalues**. Eigenvectors and eigenvalues are useful for some machine learning algorithms, but the concepts take time to solidly grasp. They are used extensively in machine learning and in this course we will encounter them in relation to Principal Components Analysis (PCA), clustering algorithms, For an intuitive review of these concepts, explore this [interactive website at Setosa.io](http://setosa.io/ev/eigenvectors-and-eigenvalues/). Also, the series of linear algebra videos by Grant Sanderson of 3Brown1Blue are excellent and can be viewed on youtube [here](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab). For these questions, numpy may once again be helpful.\n",
        "\n",
        "1. Calculate the eigenvalues and corresponding eigenvectors of matrix $\\mathbf{A}$ above, from the last question.\n",
        "2. Choose one of the eigenvector/eigenvalue pairs, $\\mathbf{v}$ and $\\lambda$, and show that $\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}$. This relationship extends to higher orders: $\\mathbf{A} \\mathbf{A} \\mathbf{v} = \\lambda^2 \\mathbf{v}$\n",
        "3. Show that the eigenvectors are orthogonal to one another (e.g. their inner product is zero). This is true for eigenvectors from real, symmetric matrices. In three dimensions or less, this means that the eigenvectors are perpendicular to each other. Typically we use the orthogonal basis of our standard x, y, and z, Cartesian coordinates, which allows us, if we combine them linearly, to represent any point in a 3D space. But any three orthogonal vectors can do the same. We will see this property is used in PCA to identify the dimensions of greatest variation in our data when we discuss dimensionality reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kS629PqTZAD"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJo_2az4TZAD"
      },
      "source": [
        "# Numerical Programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeMy78fGTZAD"
      },
      "source": [
        "## 7\n",
        "**[10 points]** Loading data and gathering insights from a real dataset\n",
        "\n",
        "In data science, we often need to have a sense of the idiosyncrasies of the data, how they relate to the questions we are trying to answer, and to use that information to help us to determine what approach, such as machine learning, we may need to apply to achieve our goal. This exercise provides practice in exploring a dataset and answering question that might arise from applications related to the data.\n",
        "\n",
        "**Data**. The data for this problem can be found in the `data` subfolder in the `assignments` folder on [github](https://github.com/kylebradbury/ids705). The filename is `a1_egrid2016.xlsx`. This dataset is the Environmental Protection Agency's (EPA) [Emissions & Generation Resource Integrated Database (eGRID)](https://www.epa.gov/energy/emissions-generation-resource-integrated-database-egrid) containing information about all power plants in the United States, the amount of generation they produce, what fuel they use, the location of the plant, and many more quantities. We'll be using a subset of those data.\n",
        "\n",
        "The fields we'll be using include:\n",
        "    \n",
        "|field    |description|\n",
        "|:-----   |:-----|\n",
        "|SEQPLT16 |eGRID2016 Plant file sequence number (the index)|\n",
        "|PSTATABB |Plant state abbreviation|\n",
        "|PNAME    |Plant name |\n",
        "|LAT      |Plant latitude |\n",
        "|LON      |Plant longitude|\n",
        "|PLPRMFL  |Plant primary fuel |\n",
        "|CAPFAC   |Plant capacity factor |\n",
        "|NAMEPCAP |Plant nameplate capacity (Megawatts MW)|\n",
        "|PLNGENAN |Plant annual net generation (Megawatt-hours MWh)|\n",
        "|PLCO2EQA |Plant annual CO2 equivalent emissions (tons)|\n",
        "\n",
        "For more details on the data, you can refer to the [eGrid technical documents](https://www.epa.gov/sites/default/files/2021-02/documents/egrid2019_technical_guide.pdf). For example, you may want to review page 45 and the section \"Plant Primary Fuel (PLPRMFL)\", which gives the full names of the fuel types including WND for wind, NG for natural gas, BIT for Bituminous coal, etc.\n",
        "\n",
        "There also are a couple of \"gotchas\" to watch out for with this dataset:\n",
        "- The headers are on the second row and you'll want to ignore the first row (they're more detailed descriptions of the headers).\n",
        "- NaN values represent blanks in the data. These will appear regularly in real-world data, so getting experience working with it will be important.\n",
        "\n",
        "**Your objective**. For this dataset, your goal is answer the following questions about electricity generation in the United States:\n",
        "\n",
        "**(a)** Which plant has generated the most energy (measured in MWh)?\n",
        "\n",
        "**(b)** What is the name of the northern-most power plant in the United States?\n",
        "\n",
        "**(c)** What is the state where the northern-most power plant in the United States is located?\n",
        "\n",
        "**(d)** Plot a bar plot showing the amount of energy produced by each fuel type across all plants.\n",
        "\n",
        "**(e)** From the plot in (d), which fuel for generation produces the most energy (MWh) in the United States?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFX1SSqATZAD"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPacePK0TZAD"
      },
      "source": [
        "## 8\n",
        "**[6 points]** *Vectorization*. When we first learn to code and think about iterating over an array, we often use loops. If implemented correctly, that does the trick. In machine learning, we iterate over so much data that those loops can lead to significant slow downs if they are not computationally efficient. In Python, vectorizing code and relying on matrix operations with efficient tools like numpy is typically the faster approach. Of course, numpy relies on loops to complete the computation, but this is at a lower level of programming (typically in C), and therefore is much more efficient. This exercise will explore the benefits of vectorization. Since many machine learning techniques rely on matrix operations, it's helpful to begin thinking about implementing algorithms using vector forms.\n",
        "\n",
        "Begin by creating an array of 1 million random numbers using the numpy `random.randn` module. Compute the sum of the squares of those random numbers first in a for loop, then using Numpy's `dot` module to perform an inner (dot) product. Time how long it takes to compute each and report the results and report the output. How many times faster is the vectorized code than the for loop approach? (Note - your results may vary from run to run).\n",
        "\n",
        "Your output should use the `print()` function as follows (where the # symbols represent your answers, to a reasonable precision of 4-5 significant figures):\n",
        "\n",
        "`Time [sec] (non-vectorized): ######`\n",
        "\n",
        "`Time [sec] (vectorized):     ######`\n",
        "\n",
        "`The vectorized code is ##### times faster than the nonvectorized code`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWd6p9zNTZAD"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lc6Nf-BTZAD"
      },
      "source": [
        "## 9\n",
        "**[10 points]** This exercise will walk through some basic numerical programming and probabilistic thinking exercises, two skills which are frequently use in machine learning for answering questions from our data.\n",
        "1. Synthesize $n=10^4$ normally distributed data points with mean $\\mu=2$ and a standard deviation of $\\sigma=1$. Call these observations from a random variable $X$, and call the vector of observations that you generate, $\\textbf{x}$.\n",
        "2. Calculate the mean and standard deviation of $\\textbf{x}$ to validate (1) and provide the result to a precision of four significant figures.\n",
        "3. Plot a histogram of the data in $\\textbf{x}$ with 30 bins\n",
        "4. What is the 90th percentile of $\\textbf{x}$? The 90th percentile is the value below which 90% of observations can be found.\n",
        "5. What is the 99th percentile of $\\textbf{x}$?\n",
        "6. Now synthesize $n=10^4$ normally distributed data points with mean $\\mu=0$ and a standard deviation of $\\sigma=3$. Call these observations from a random variable $Y$, and call the vector of observations that you generate, $\\textbf{y}$.\n",
        "7. Create a new figure and plot the histogram of the data in $\\textbf{y}$ on the same axes with the histogram of $\\textbf{x}$, so that both histograms can be seen and compared.\n",
        "8. Using the observations from $\\textbf{x}$ and $\\textbf{y}$, estimate $E[XY]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrfTIgMmTZAD"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uENxx9_VTZAD"
      },
      "source": [
        "# Version Control via Git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shzvGVzgTZAE"
      },
      "source": [
        "## 10\n",
        "**[4 points]** Git is efficient for collaboration, and expectation in industry, and one of the best ways to share results in academia. You can even use some Git repositories (e.g. Github) as hosts for website, such as with the [course website](https://kylebradbury.github.io/ids705/index.html). As a data scientist with experience in machine learning, Git is expected. We will interact with Git repositories (a.k.a. repos) throughout this course, and your project will require the use of git repos for collaboration.\n",
        "\n",
        "Complete the [Atlassian Git tutorial](https://www.atlassian.com/git/tutorials/what-is-version-control), specifically the following listed sections. Try each concept that's presented. For this tutorial, instead of using BitBucket as your remote repository host, you may use your preferred platform such as [Github](https://github.com/) or [Duke's Gitlab](https://gitlab.oit.duke.edu/users/sign_in).\n",
        "1. [What is version control](https://www.atlassian.com/git/tutorials/what-is-version-control)\n",
        "2. [What is Git](https://www.atlassian.com/git/tutorials/what-is-git)\n",
        "3. [Install Git](https://www.atlassian.com/git/tutorials/install-git)\n",
        "4. [Setting up a repository](https://www.atlassian.com/git/tutorials/install-git)\n",
        "5. [Saving changes](https://www.atlassian.com/git/tutorials/saving-changes)\n",
        "6. [Inspecting a repository](https://www.atlassian.com/git/tutorials/inspecting-a-repository)\n",
        "7. [Undoing changes](https://www.atlassian.com/git/tutorials/undoing-changes)\n",
        "8. [Rewriting history](https://www.atlassian.com/git/tutorials/rewriting-history)\n",
        "9. [Syncing](https://www.atlassian.com/git/tutorials/syncing)\n",
        "10. [Making a pull request](https://www.atlassian.com/git/tutorials/making-a-pull-request)\n",
        "11. [Using branches](https://www.atlassian.com/git/tutorials/using-branches)\n",
        "12. [Comparing workflows](https://www.atlassian.com/git/tutorials/comparing-workflows)\n",
        "\n",
        "I also have created two videos on the topic to help you understand some of these concepts: [Git basics](https://www.youtube.com/watch?v=fBCwfoBr2ng) and a [step-by-step tutorial](https://www.youtube.com/watch?v=nH7qJHx-h5s).\n",
        "\n",
        "For your answer, affirm that you *either* completed the tutorials above OR have previous experience with ALL of the concepts above. Confirm this by typing your name below and selecting the situation that applies from the two options in brackets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BASw0yPhTZAE"
      },
      "source": [
        "**ANSWER**\n",
        "\n",
        "*I, [**your name here**], affirm that I have [**completed the above tutorial / I have previous experience that covers all the content in this tutorial**]*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip7LwEpCTZAE"
      },
      "source": [
        "# Exploratory Data Analysis\n",
        "## 11\n",
        "**[20 points]** Here you'll bring together some of the individual skills that you demonstrated above and create a Jupyter notebook based blog post on your exploratory data analysis. Your goal is to identify a question or problem and to work towards solving it or providing additional information or evidence (data) related to it through your data analysis. Below, we walk through a process to follow for your analysis. Additionally, you can find an [example of a well-done exploratory data analysis here from past years](https://github.com/kylebradbury/ids705/blob/master/assignments/Assignment_1_Q11_Example.ipynb).\n",
        "\n",
        "1. Find a dataset that interests you and relates to a question or problem that you find intriguing.\n",
        "2. Describe the dataset, the source of the data, and the reason the dataset was of interest. What question are you hoping to answer through exploring the dataset?\n",
        "3. Check the data and see if they need to be cleaned: are there missing values? Are there clearly erroneous values? Do two tables need to be merged together? Clean the data so it can be visualized. If the data are clean, state how you know they are clean (what did you check?).\n",
        "3. Plot the data, demonstrating interesting features that you discover. Are there any relationships between variables that were surprising or patterns that emerged? Please exercise creativity and curiosity in your plots. You should have at least a ~3 plots exploring the data in different ways.\n",
        "4. What insights are you able to take away from exploring the data? Is there a reason why analyzing the dataset you chose is particularly interesting or important? Summarize this for a general audience (imagine your publishing a blog post online) - boil down your findings in a way that is accessible, but still accurate.\n",
        "\n",
        "Here your analysis will evaluated based on:\n",
        "1. Motivation: was the purpose of the choice of data clearly articulated? Why was the dataset chosen and what was the goal of the analysis?\n",
        "2. Data cleaning: were any issues with the data investigated and, if found, were they resolved?\n",
        "3. Quality of data exploration: were at least 4 unique plots (minimum) included and did those plots demonstrate interesting aspects of the data? Was there a clear purpose and takeaway from EACH plot?\n",
        "4. Interpretation: Were the insights revealed through the analysis and their potential implications clearly explained? Was there an overall conclusion to the analysis?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l56U-8r9TZAE"
      },
      "source": [
        "**ANSWER**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "nteract": {
      "version": "0.28.0"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "643px",
        "left": "1548px",
        "right": "20px",
        "top": "121px",
        "width": "350px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}